import sagemaker
from sagemaker.serverless import ServerlessInferenceConfig
from sagemaker.pytorch import PyTorchModel
import tarfile
import os
import logging
from dotenv import load_dotenv

# .envãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’ç’°å¢ƒå¤‰æ•°ã¨ã—ã¦èª­ã¿è¾¼ã‚€
load_dotenv()

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def create_script_tar(source_dir: str = './app', output_file: str = 'script.tar.gz'):
    """
    Create tar.gz file from ./app directory.
    """
    logger.info(f"Creating {output_file} from {source_dir}...")
    
    if not os.path.exists(source_dir):
        raise FileNotFoundError(f"Source directory not found: {source_dir}")
    
    with tarfile.open(output_file, 'w:gz') as tar:
        # Add all files from app directory
        for root, dirs, files in os.walk(source_dir):
            for file in files:
                file_path = os.path.join(root, file)
                # Calculate arcname to maintain directory structure
                arcname = os.path.relpath(file_path, source_dir)
                tar.add(file_path, arcname=arcname)
                logger.info(f"  Added: {arcname}")
    
    file_size_mb = os.path.getsize(output_file) / (1024 * 1024)
    logger.info(f"âœ… Created {output_file} ({file_size_mb:.2f} MB)")
    return output_file

# SageMakerã‚»ãƒƒã‚·ãƒ§ãƒ³ã¨ãƒ­ãƒ¼ãƒ«ã‚’åˆæœŸåŒ–
sagemaker_session = sagemaker.Session()
role = os.getenv('AWS_SAGEMAKER_ROLE')

# ./appãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’script.tar.gzã«åœ§ç¸®
script_tar_path = create_script_tar('./app', 'script.tar.gz')

# --- äº‹å‰ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«S3ãƒ‘ã‚¹ã‚’æŒ‡å®š ---
model_s3_uri = 's3://climbinsight-ai-models/sam-models/sam_vit_b.tar.gz'

# script.tar.gzã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
source_dir_path = sagemaker_session.upload_data(
    path=script_tar_path,
    bucket=sagemaker_session.default_bucket(),
    key_prefix='script-source-dir'
)

logger.info(f"Script uploaded to: {source_dir_path}")

# ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
model = PyTorchModel(
    model_data=model_s3_uri,
    role=role,
    framework_version='2.6',
    py_version='py312',
    sagemaker_session=sagemaker_session,
    source_dir=source_dir_path,
    entry_point='inference.py'
)

# ã‚µãƒ¼ãƒãƒ¬ã‚¹è¨­å®š
serverless_config = ServerlessInferenceConfig(
    memory_size_in_mb=6144,
    max_concurrency=1
)

# ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®ãƒ‡ãƒ—ãƒ­ã‚¤
logger.info("ğŸš€ Deploying model to SageMaker endpoint...")
predictor = model.deploy(
    serverless_inference_config=serverless_config,
    endpoint_name='image-process-endpoint'
)

logger.info(f"ğŸ‰ Deployment completed successfully!")
logger.info(f"ğŸ“ Endpoint name: {predictor.endpoint_name}")

# ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—: ãƒ­ãƒ¼ã‚«ãƒ«ã®tar.gzãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
try:
    os.remove(script_tar_path)
    logger.info(f"ğŸ—‘ï¸  Cleaned up local file: {script_tar_path}")
except Exception as e:
    logger.warning(f"âš ï¸  Could not remove {script_tar_path}: {str(e)}")

print(f"Endpoint name: {predictor.endpoint_name}")