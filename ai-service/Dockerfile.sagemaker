# Multi-stage Dockerfile optimized for SageMaker deployment
# Base image with CUDA support for PyTorch
FROM pytorch/pytorch:2.1.0-cuda11.8-cudnn8-runtime

# Set environment variables
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1
ENV SAGEMAKER_PROGRAM=inference.py

# Install system dependencies
RUN apt-get update && apt-get install -y \
    libglib2.0-0 \
    libsm6 \
    libxext6 \
    libxrender-dev \
    libgomp1 \
    libgl1-mesa-glx \
    libglib2.0-0 \
    wget \
    git \
    && rm -rf /var/lib/apt/lists/*

# Set working directory
WORKDIR /opt/ml/code

# Copy requirements and install dependencies
COPY requirements-sagemaker.txt .
RUN pip install --no-cache-dir -r requirements-sagemaker.txt

# Install SageMaker inference toolkit
RUN pip install --no-cache-dir sagemaker-inference==1.9.0

# Copy application code
COPY inference.py .
COPY app/sam.py ./sam.py

# Create necessary directories for SageMaker
RUN mkdir -p /opt/ml/model
RUN mkdir -p /opt/ml/input
RUN mkdir -p /opt/ml/output

# Set permissions
RUN chmod +x inference.py

# Environment variable for SageMaker model server
ENV SAGEMAKER_SUBMIT_DIRECTORY=/opt/ml/code

# SageMaker uses PORT 8080 by default
EXPOSE 8080

# Default command for SageMaker serving
# SageMaker will override this with its own serving command
CMD ["python", "inference.py"]