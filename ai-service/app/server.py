import os
import threading
from flask import Flask
from waitress import serve
import grpc
from concurrent import futures
import time
from dotenv import load_dotenv

import ai_pb2
import ai_pb2_grpc

from sam import load_sam_model, process_image_bytes, Coordinate

MAX_MESSAGE_LENGTH = 20 * 1024 * 1024  # 20MB ã«å¢—ã‚„ã™ãªã©

class AIService(ai_pb2_grpc.AIServiceServicer):
    def __init__(self):
        load_dotenv()
        self.predictor = None

    def Process(self, request, context):
        print(f"ğŸ“¥ å—ä¿¡")
        if self.predictor is None:
            print("ğŸ§  SAM ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ä¸­...")
            self.predictor = load_sam_model()
            print("âœ… ãƒ¢ãƒ‡ãƒ«æº–å‚™å®Œäº†")
        p = [Coordinate(x=p.x, y=p.y) for p in request.points]
        result_bytes = process_image_bytes(request.image, p, self.predictor)
        print(f"å‡¦ç†å®Œäº†")
        return ai_pb2.ProcessImageResponse(processed_image=result_bytes, mime_type="image/png")
    
def serve_grpc():
    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10),
                         options=[
                             ("grpc.max_receive_message_length", MAX_MESSAGE_LENGTH),
                             ("grpc.max_send_message_length", MAX_MESSAGE_LENGTH)])
    ai_pb2_grpc.add_AIServiceServicer_to_server(AIService(), server)

    port = 50051
    server.add_insecure_port(f'[::]:{port}')
    server.start()
    print(f"ğŸš€ gRPCã‚µãƒ¼ãƒãƒ¼èµ·å‹•ä¸­... ãƒãƒ¼ãƒˆ: {port}")
    try:
        while True:
            time.sleep(86400)  # ã‚µãƒ¼ãƒãƒ¼ã‚’å¸¸ã«ç¶­æŒ
    except KeyboardInterrupt:
        print("ğŸ›‘ ã‚µãƒ¼ãƒãƒ¼åœæ­¢")
        server.stop(0)


def serve_http():
    app = Flask(__name__)

    @app.route('/healthz')
    def health_check():
        return "OK", 200

    port = int(os.getenv('PORT', 8090))
    print(f"ğŸŒ HTTPãƒ˜ãƒ«ã‚¹ã‚µãƒ¼ãƒãƒ¼èµ·å‹•ä¸­... ãƒãƒ¼ãƒˆ: {port}")
    serve(app, host='0.0.0.0', port=port)


if __name__ == '__main__':
    threading.Thread(target=serve_grpc, daemon=True).start()
    serve_http()